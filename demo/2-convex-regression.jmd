---
title: Example 2, Convex Regression
weave_options:
    fig_ext: .png
---

```{julia}
using ProximalDistanceAlgorithms
using LinearAlgebra, Random, StableRNGs, Plots, DataFrames
include(joinpath(dirname(@__DIR__), "plotutils.jl"))
gr(linewidth=3, dpi=120)
```

### Introduction

```{julia}
@doc cvxreg_fit
```

### Example

```{julia}
φ(x) = dot(x, x)
d = 1       # number of covariates
n = 50      # number of samples
σ = 0.1     # std dev in noise
Random.seed!(512)
response, truth, covariates = cvxreg_example(φ, d, n, σ, rng=StableRNG(512))
fig = plot(xlabel = "x", ylabel = "phi(x)")
plot!(fig, vec(covariates), truth, label = "ground truth")
scatter!(fig, vec(covariates), response, label = "observed", markersize = 4)
```

#### Setup

```{julia}
function solve(response, covariates, algorithm, maxiters, penalty)
    accel = Val(:nesterov)
    gtol, dtol, rtol = 1e-3, 1e-2, 1e-6

    if algorithm isa SteepestDescent
        println("Steepest Descent + Nesterov")

        # track loss, penalized objective, gradient, etc.
        history, logger = initialize_history(maxiters)

        # warm-up
        println(" | warm-up:")
        print(" | ")
        @time cvxreg_fit(algorithm, response, covariates,
            nouter   = 2,
            ninner   = 10,
            penalty  = penalty,
            accel    = accel,
            delay    = 0,
            callback = logger,
            gtol     = gtol,
            dtol     = dtol,
            rtol     = rtol,)

        # real timing
        println(" | result:")
        print(" | ")
        history, logger = initialize_history(maxiters)
        solution, _ = @time cvxreg_fit(algorithm, response, covariates,
            nouter   = maxiters,
            ninner   = 10^5,
            penalty  = penalty,
            accel    = accel,
            delay    = 0,
            callback = logger,
            gtol     = gtol,
            dtol     = dtol,
            rtol     = rtol,)
    else
        algstr = algorithm isa MM ? "MM + Nesterov" : "ADMM"
        println(algstr)

        # track loss, penalized objective, gradient, etc.
        h1, l1 = initialize_history(maxiters)
        h2, l2 = initialize_history(maxiters)
        history = (lsqr = h1, cg = h2)

        # store solutions
        sol = []

        for (ls, h, l) in zip((Val(:LSQR), Val(:CG)), (h1, h2), (l1, l2))
            lsstr = ls isa Val{:LSQR} ? "LSQR"  : "CG"
            println(" |")
            println(" | linear solver: $(lsstr)")

            # warm-up
            println(" | warm-up:")
            print(" | ")
            _, _l = initialize_history(maxiters+1)
            @time cvxreg_fit(algorithm, response, covariates,
                nouter   = 2,
                ninner   = 10,
                penalty  = penalty,
                accel    = accel,
                delay    = 0,
                callback = _l,
                ls       = ls,
                gtol     = gtol,
                dtol     = dtol,
                rtol     = rtol,)

            # real timing
            println(" | result:")
            print(" | ")
            s, _ = @time cvxreg_fit(algorithm, response, covariates,
                nouter   = maxiters,
                ninner   = 10^5,
                penalty  = penalty,
                accel    = accel,
                delay    = 0,
                callback = l,
                ls       = ls,
                gtol     = gtol,
                dtol     = dtol,
                rtol     = rtol,)

            push!(sol, s)
        end

        solution = (lsqr=sol[1], cg=sol[2])
    end

    return solution, history
end
```

#### Annealing schedules

```{julia}
penalty(ρ, n) = 1.1*ρ

maxiters = 100
xs = 0:maxiters
ys = [1.0]; foreach(i -> push!(ys, penalty(ys[end], i)), xs[2:end])
plot(xs, ys, yscale=:log10, legend = nothing)
xlabel!("iteration")
ylabel!("rho")
```

##### MM

```{julia}
MM_sol, MM_trace = solve(response, covariates, MM(), maxiters, penalty)
plot_summary(MM_trace.lsqr)
```

```{julia}
plot_summary(MM_trace.cg)
```

### Steepest Descent

```{julia}
SD_sol, SD_trace = solve(response, covariates, SteepestDescent(), maxiters, penalty);
plot_summary(SD_trace)
```

### ADMM

```{julia}
ADMM_sol, ADMM_trace = solve(response, covariates, ADMM(), maxiters, penalty)
plot_summary(ADMM_trace.lsqr)
```

```{julia}
plot_summary(ADMM_trace.cg)
```

### Quality of solutions

```{julia}
using Statistics
algorithm = ["MM", "SD", "ADMM"]
traces = (MM_trace, SD_trace, ADMM_trace)
df = table_summary(traces..., algname = algorithm)

df[!, :MSE] = [
    mean((MM_sol.lsqr    .- truth) .^ 2),
    mean((MM_sol.cg      .- truth) .^ 2),
    mean((SD_sol         .- truth) .^ 2),
    mean((ADMM_sol.lsqr  .- truth) .^ 2),
    mean((ADMM_sol.cg    .- truth) .^ 2),
]
df
```

### Appendix

```{julia}
using Pkg; Pkg.status()
```

```{julia}
using InteractiveUtils; versioninfo()
```
