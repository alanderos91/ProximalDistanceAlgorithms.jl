\documentclass[11pt]{article}

\input{preamble.tex}
\usepackage{hyperref}

\title{Proximal Distance Application: Convex Clustering}
\author{Alfonso Landeros}
\date{\today}

\begin{document}
\maketitle

Convex clustering of \(n\) samples based on \(d\) features can be formulated in terms of the regularized objective
\begin{equation}
    \label{eq:regularized-objective}
    F_{\gamma}(\bU)
    =
    \frac{1}{2} \|\bU - \bX\|_{F}^{2}
    +
    \gamma \sum_{i < j} w_{ij} \|\bU (\be_{i} - \be_{j})\|,
\end{equation}
where \(\bX \in \Real^{d \times n}\) encodes the data, columns of \(\bU \in \Real^{d \times n}\) represent cluster assignments, and \(\be_{k} \in \Real^{n}\) is the standard basis vector.
The weights \(w_{ij}\) have a graphical interpretation.
Related samples have \(w_{ij} > 0\), otherwise \(w_{ij} = 0\).
This implies that minimization of \(F_{\gamma}(\bU)\) separates over the connected components of the graph.
Finally, the regularization parameter \(\gamma\) tunes the number of clusters in a non-linear fashion.
Previous work establishes that the solution path \(\bU(\gamma)\) varies continuously with respect to regularization \cite{chi2015}.

The convex clustering problem can be solved with an ADMM approach, or its relative the AMA.
These notes seek to reformulate convex clustering as a projection problem.
Enforcing a small number of clusters amounts to strong consensus in columns of \(\bU\); that is, \(\bu_{i} = \bu_{j}\) for all but a few pairs \((i,j)\).
An equivalent approach is to impose \textit{block} sparsity on a vector (or matrix) encoding the differences \(\bu_{i} - \bu_{j}\).
The advantages of the projection perspective include access to proximal distance methods and an explicit mechanism for selecting the number of clusters.

Throughout these notes, \(d\) denotes the number of features and \(n\) the number of samples.
We use \(m = \binom{n}{2}\) as shorthand for the number of comparisons \(\bu_{i} - \bu_{j}\).

\section*{\center The Fusion Matrix and Distance Majorizaiton}

Here we derive a linear operator \(\bD\) mapping columns of \(\bU\) to the differences \(\bu_{i} - \bu_{j}\).
This operator will be referred to as \textit{the fusion matrix} for the convex clustering problem.

To start, define a collection of comparison matrices \(\bD^{i,j} \in \Real^{d \times dn}\) by the rule
\begin{equation}
    \label{eq:comparison-matrix-1}
    \bD^{i,j}[\vec(\bU)]
    =
    \bu_{i} - \bu_{j}
    =
    \bU (\be_{i} - \be_{j})
\end{equation}
with \(\vec(\bU) \in \Real^{dn}\).
Note that the vectorization operation maps column \(\bu_{j}\) to the linear indices \((j-1)n + 1\) up to \((j-1)n + d\).
Thus, define the index sets
\begin{equation*}
    \label{eq:index-sets}
    \mathcal{I}_{j}^{n}
    =
    \{k : (j-1)n + 1 \le k \le (j-1)n + d\}
\end{equation*}
so that rows of \(\bD^{i,j}\) take the form
\begin{equation}
    \label{eq:comparison-matrix-2}
    D_{k\ell}^{i,j}
    =
    \begin{cases}
        +1, & \text{if \(\ell \in \mathcal{I}_{i}^{n}\)} \\
        -1, & \text{if \(\ell \in \mathcal{I}_{j}^{n}\)} \\
        \phantom{+}0, & \text{otherwise}
    \end{cases}
    \implies
    \bD^{i,j}
    =
    (\be_{i}^{t} \otimes \bI_{d})
    -
    (\be_{j}^{t} \otimes \bI_{d})
\end{equation}
Note that \(\mathcal{I}_{i}^{n} \cap \mathcal{I}_{j}^{n} = \emptyset\) whenever \(i \neq j\).
Identify the fusion matrix by stacking the \(m = \binom{n}{2}\) comparison matrices defined in (\ref{eq:comparison-matrix-1})-(\ref{eq:comparison-matrix-2}) (here given in terms of its transpose):
\begin{equation}
    \label{eq:fusion-matrix}
    \bD^{t}
    =
    \begin{bmatrix}
        \bD^{1,2}
        & \cdots
        & \bD^{i,j}
        & \cdots
        & \bD^{n-1,n}
    \end{bmatrix}^{t}
    \in
    \Real^{dn \times dm}.
\end{equation}
This result is easier to derive by applying the ``vec'' identity
\begin{equation*}
    \vec[\bU (\be_{i} - \be_{j})]
    =
    \left[(\be_{i} - \be_{j})^{t} \otimes \bI_{d}\right] \vec(\bU)
    =
    \bD^{i,j} \vec(\bU).
\end{equation*}
We are now in a position to attack the convex clustering problem with a distance penalty.
Letting \(S_{k}\) denote the set of \(k\)-block sparse vectors, we propose minimizing the penalized objective
\begin{equation}
    \label{eq:penalized-loss}
    h_{\rho}(\bU)
    =
    \frac{1}{2}\|\bU - \bX\|_{F}^{2}
    +
    \frac{\rho}{2} \dist(\bW \bD \vec(\bU), S_{k})^{2},
\end{equation}
where the diagonal matrix \(\bW\) which scales blocks corresponding to each difference \(\bu_{i} - \bu_{j}\).
Concretely, its structure is
\begin{equation*}
    \bW
    =
    \begin{bmatrix}
        w_{12} \bI_{d} & 0 & 0 & \cdots & 0 \\
        0 & w_{13} \bI_{d} & 0 & \cdots & 0 \\
        0 & 0 & w_{14} \bI_{d} & \ddots & \vdots \\
        \vdots & \vdots & \ddots & \ddots & 0\\
        0 & 0 & \cdots & 0 & w_{n-1~n} \bI_{d}
    \end{bmatrix}.
\end{equation*}
Take \(\bu = \vec(\bU)\) and \(\bx = \vec(\bX)\), and apply a distance majorization to derive the handy surrogate
\begin{equation}
    \label{eq:surrogate}
    g_{\rho}(\bu \mid \bu_{n})
    =
    \frac{1}{2}\|\bu - \bx\|_{2}^{2}
    +
    \frac{\rho}{2}
    \| \bW \bD \bu - P_{S_{k}}(\bW \bD \bu_{n}) \|_{2}^{2}.
\end{equation}
Its gradient is affine in \(\bu\):
\begin{equation}
    \label{eq:gradient}
    \bv_{n}
    \equiv
    \nabla g_{\rho}(\bu \mid \bu_{n})
    =
    (\bu - \bx)
    +
    \rho \bD^{t} \bW^{t} \left[
        \bW \bD \bu - P_{S_{k}}(\bW \bD \bu_{n})
    \right].
\end{equation}

\section*{\center Matrix-Free Operations}

The constraint matrix has dimensions \(dm \times dn\) where \(m = \binom{n}{2}\) so we should avoid forming linear operators as matrices wherever possible.
Equation (\ref{eq:gradient}) suggests that any first-order method needs to handle four main operations: (i) left-multiplication by \(\bD^{t} \bW^{t} \bW \bD\), (ii) left-multiplication by \(\bW \bD\), (iii) left-multiplication by \(\bD^{t} \bW^{t}\), and (iv) projection onto a block sparsity set \(P_{S_{k}}(\cdot)\).
Here we derive explicit formulas for each operation and discuss their interpretation along with computational complexity.

\subsection*{(i) Operator \(\bD^{t} \bW^{t} \bW \bD\): Weighted Averaging + Centering}

It is easy to see that \(\bW^{t} \bW\) will simply apply squared weights \(w_{ij}^{2}\) to blocks encoding \(\bu_{i} - \bu_{j}\).
Applying \(\bD^{t} \bW^{t} \bW \bD\) to \(\bu = \vec(\bU)\) is effectively an averaging operation.
Through abuse of notation, say \(w_{ij} = w_{ji}\) and define
\begin{align*}
    \bar{w}_{i}^{2}
    &=
    \frac{1}{n-1} \sum_{j \neq i}^{n} w_{ij}^{2}
    \qquad
    \text{(related to weighted degree of point \(i\))} \\
    \tilde{\bu}_{i}
    &=
    \frac{1}{n-1} \sum_{j \neq i}^{n} w_{ij}^{2} \bu_{j}
    \quad
    \text{(leave-one-out weighted arithmetic mean)}
\end{align*}
which will feature in our derivation.
Passing to the unvectorized version of the product, we have
\begin{align*}
    \sum_{i < j} w_{ij}^{2}~\bU (\be_{i} - \be_{j}) (\be_{i} - \be_{j})^{t}
    &=
    2 \sum_{i=1}^{n} \sum_{j \neq i}^{n} w_{ij}^{2} (\bu_{i} - \bu_{j}) \be_{i}^{t} \\
    &=
    2 (n-1) \sum_{i=1}^{n} (\bar{w}_{i} \bu_{i} - \tilde{\bu}_{i}) \be_{i}^{t} \\
    &=
    2 (n-1) \begin{bmatrix}
        (\bar{w}_{1} \bu_{1} - \tilde{\bu}_{1})
        & (\bar{w}_{2} \bu_{2} - \tilde{\bu}_{2})
        & \cdots
        & (\bar{w}_{n} \bu_{n} - \tilde{\bu}_{n})
    \end{bmatrix}
\end{align*}
We exploit the symmetric roles of \((i,j)\) in passing to the first equality.
The rest follows from applying the definitions above and concatenating column vectors.
The weights \(\bar{w}_{i}\) can be computed once offline.
Computing the required averages and centering operations can be done in \(\mathcal{O}([dn]^{2})\) which substantially better than a naive approach with \(\mathcal{O}(d^{2}mn) \sim \mathcal{O}(d^{2}n^{3})\) FLOPS.
Sparse matrix implementation can match the computational complexity of the explicit formula but incurs overhead from chasing pointers.
Further, this approach can be parallized especially in the Julia language.

\subsection*{(ii) Operator \(\bW \bD\): Weighted Differencing}

This operation is defined to be a weighted differencing and obviously does not need to be implemented with explicit matrix-vector multiplication.
Its computational complexity is \(\mathcal{O}(n^{2})\) due to the number of comparisons.

\subsection*{(iii) Operator \(\bD^{t} \bW^{t}\): Sparse Correction}

We can focus soley on \(\bD^{t}\) because \(\bW\) is block diagonal.
Letting \(\by_{n} = P_{S_{k}}(\bD \bu_{n})\), the notation \(\by^{i,j}_{n}\) indicates the block corresponding to the projection of the difference \(\bu_{n,i} - \bu_{n,j}\).
Typically \(k\) is small so \(\by_{n}\) is highly sparse.
From (\ref{eq:fusion-matrix}) it follows that the expression \(\bz + \bD^{t} \by_{n}\) translates to the sum
\begin{equation*}
    \bz + \sum_{i<j} [(\be_{i} - \be_{j}) \otimes \bI_{d}]~\by^{i,j}_{n},
\end{equation*}
which (i) adds \(\by^{i,j}_{n}\) to block \(i\) of \(\bz\) and (ii) subtracts \(\by^{i,j}_{n}\) from block \(j\) of \(\bz\).
Thus, the computational complexity of \(\bz + \bD^{t} \by_{n}\) is \(\mathcal{O}(k)\).

\subsection*{(iv) Operator \(P_{S_{k}}(\cdot)\): Block Sparsity Projection}

Projection onto \(S_{k}\) is a simple operation.
We start with a motivating example with blocks of size 1.
Suppose we want a \(2\)-sparse representation of a vector \(\bx \in \Real^{5}\).
In this setting, we should keep that two largest entries of \(\bx\) and drop the rest in order to remain ``close'' to the original vector.
For example,
\begin{equation*}
    \begin{bmatrix}
        1 & 2 & 3 & 4 & 2
    \end{bmatrix}
    \overset{P_{S_{k}}}{\longrightarrow}
    \begin{bmatrix}
        0 & 0 & 3 & 4 & 0
    \end{bmatrix}
\end{equation*}
so that the distance between the two vectors is \(\sqrt{5}\).
Keeping the smallest components tends to increase the distance; see for example
\begin{equation*}
    \begin{bmatrix}
        1 & 2 & 3 & 4 & 2
    \end{bmatrix}
    {\longrightarrow}
    \begin{bmatrix}
        1 & 2 & 0 & 0 & 0
    \end{bmatrix}
\end{equation*}
which implies distance \(5\).
Extending this idea to structured sparsity requires us to have a notion of ``small blocks''.
A natural choice is to impose a norm condition.
For example, under a norm condition we should keep the top $k$ blocks based on \(\|\bu_{i} - \bu_{j}\|_{\dagger}\) and set the rest to zero.
This procedure effectively selects $k$ representatives for each cluster and assigns the remaining $n - k$ points to one of the \(k\) groups.
Generally, the objectives (\ref{eq:regularized-objective}) and (\ref{eq:penalized-loss}) impose additional weights on each norm of differences, but this adds little additional overhead.
Finding the largest \(k\) distances \(w_{ij} \| \bu_{i} - \bu_{j} \|\) can be achieved with a single sweep through the data.
Accounting for comparisons, computing each norm, and tracking the \(k\) largest values implies a complexity of order \(\mathcal{O}(\log(k)dn^{2})\).

\subsection*{A Few Observations}

\begin{itemize}
    \item Combining (iii) and (iv) is straightforward.
    Moreover, these notes suggest there is no need to compute \(P_{S_{k}}(\bD \bu_{n})\).
    It is enough to identify the top \(k\) blocks required for the projection so that the ``sparse correction'' can be applied without explicit calls to linear algebra subroutines.

    \item The block sparsity projection is not unique in general.
    In this application, \(\bU\) should have \(k \ll n\) unique columns to reflect assignment to \(k\) clusters.
    The initial point \(\bU_{0} = \bX\) and subsequent iterates in our proximal distance algorithms will naturally avoid ties thanks to the properties of metrics.
    This assumes \(\bX\) has been pre-processed to identify and remove redundant samples.
\end{itemize}

\section*{\center Algorithm 1: Proximal Distance}

The surrogate (\ref{eq:surrogate}) and stationarity condition \(\bv_{n} = \boldsymbol{0}\) implied by (\ref{eq:gradient}) yield the algorithm map
\begin{equation*}
    \bu_{n+1}
    =
    \left(\frac{1}{\rho} + \bD^{t} \bW^{t} \bW \bD\right)^{-1}
    \left[\frac{1}{\rho} \bx + \bD^{t} \bW^{t} P_{S_{k}}(\bW \bD \bu_{n})\right],
\end{equation*}
which is simply solving a linear system \(\bA \bx = \bb\).
Setting up the RHS is dominated by computational cost of finding the largest \(k\) distances \(w_{ij} \| \bu_{i} - \bu_{j} \|\).
The required matrix inverse should have an explicit inverse; empirical tests suggest both \(\bA\) and \(\bA^{-1}\) are diagonal + circulant with identical sparsity patterns.

\section*{\center Algorithm 2: Proximal Distance + Steepest Descent}

An alternative first-order method is to use the gradient of the surrogate as a proxy for \(\nabla h_{\rho}(\bU)\) and implement steepest descent.
At each iteration we proceed by
\begin{equation*}
    \bu_{n+1} = \bu_{n} - \gamma_{n} \bv_{n},
\end{equation*}
where the optimal step size is
\begin{equation*}
    \gamma_{n} = \frac{\|\bv_{n}\|^{2}}{\|\bv_{n}\|^{2} + \rho \|\bW \bD \bv_{n}\|^{2}}.
\end{equation*}
Our strategy for evaluating the gradient is as follows:
\begin{enumerate}
    \item Identify the largest cluster distances based on the weighted criterion \(w_{ij} \| \bu_{i} - \bu_{j} \|\).
    
    \item Evaluate the centering operation \(\bu - \bx\), which is equivalently \(\bu_{i} - \bx_{i}\) for each block.
    
    \item Accumulate the result of the averaging + centering, \(2(n-1) \rho \left[\bar{w}_{i} \bu_{i} - \tilde{\bu}_{i}\right]\), for each block.
    
    \item Apply the ``sparsity correction'' \(w_{ij} (\bu_{i} - \bu_{j})\) to blocks \(i\) and \(j\) based on the largest cluster distances in Step 1.
\end{enumerate}

\begin{thebibliography}{1}
    \bibitem{chi2015}
    Chi, E. C., Lange, K. (2015). {Splitting Methods for Convex Clustering}. {Journal of Computational and Graphical Statistics}, 24(4), 994â€“1013. \url{https://doi.org/10.1080/10618600.2014.948181}
\end{thebibliography}
\end{document}